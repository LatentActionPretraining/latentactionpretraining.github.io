<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="icon" href="./static/images/lapa_final.ico" type="image/x-icon">
    <meta charset="utf-8">
    <title>LAPA</title>
    <meta name="description" content="LAPA: Latent Action Pretraining from Videos">
    <meta name="keywords" content="Generalization, Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <style>
        .video-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr); /* Creates a 3-column grid */
            gap: 10px; /* Adds some space between the videos */
            max-width: 1000px; /* Adjust based on your preference */
            margin: auto; /* Center the grid horizontally */
        }
        .video-grid video {
            width: 100%; /* Makes video fill the cell */
            aspect-ratio: 16 / 9; /* Keeps the aspect ratio of videos */
        }
        .video-section-header {
            text-align: center;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        #perturbation-task-video {
            width: 100%;
            height: auto;
        }
    </style>

    <script>
        function init() {
            const video = document.getElementById("perturbation-task-video");
            video.addEventListener("error", () => {
                console.log("Error loading video: ", video.src), ". Setting default to none";
                if(video.src.includes("undefined")) {
                    console.log("Don't have an undefined version of the clip, just crash");
                    return;
                }
                const task = document.getElementById("single-menu-tasks").value;
                const uri = "static/videos/perturbations/" + task + "-undefined.mp4";
                video.src = uri;
                video.playbackRate = 1.75;
                video.play();
            }, true);
        }
        function updateSingleVideo() {
            const task = document.getElementById("single-menu-tasks").value;
            const perturbation = document.getElementById("single-menu-perturbations").value;
            const video = document.getElementById("perturbation-task-video");
            const uri = "static/videos/perturbations/" + task + "-" + perturbation + ".mp4"
            video.src = uri;
            video.playbackRate = 1.75;
            video.play();
        }
    </script>
</head>

<body onload="init(); updateSingleVideo();">
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            LAPA: Latent Action Pretraining from Videos
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://seonghyeonye.github.io/">Seonghyeon Ye</a><sup>*</sup><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://joeljang.github.io/">Joel Jang</a><sup>*</sup><sup>2</sup>,
                            </span><br>
                            <span class="author-block">
                                <a target="_blank" href="https://scholar.google.com/citations?user=_Kw32VoAAAAJ&hl=ko">Byeongguk Jeon</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://scholar.google.co.kr/citations?user=xii168wAAAAJ&hl">Sejune Joo</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://jwyang.github.io/">Jianwei Yang</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl">Baolin Peng</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://ai.stanford.edu/~amandlek/">Ajay Mandlekar</a><sup>4</sup>,
                            </span><br>
                            <span class="author-block">
                                <a target="_blank" href="https://largeworldmodel.github.io/">Reuben Tan</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://research.nvidia.com/person/yu-wei-chao">Yu-Wei Chao</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://yuchenlin.xyz/">Yuchen Lin</a><sup>5</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://scholar.google.com/citations?user=r4KX3UgAAAAJ&hl">Lars Liden</a><sup>3</sup>,
                            </span><br>
                            <span class="author-block">
                                <a target="_blank" href="https://sites.google.com/view/kiminlee">Kimin Lee</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://www.microsoft.com/en-us/research/people/jfgao/?from=https://research.microsoft.com/en-us/um/people/jfgao/&type=exact">Jianfeng Gao</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>2,4</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://seominjoon.github.io/">Minjoon Seo</a><sup>1</sup>
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>KAIST</span>
                            <span class="author-block"><sup>2</sup>University of Washington</span>
                            <span class="author-block"><sup>3</sup>Microsoft Research</span>
                            <span class="author-block"><sup>4</sup>NVIDIA</span>
                            <span class="author-block"><sup>5</sup>Allen Institute for AI</span>
                        </div>
                        <div class="footnote">
                            <p>* Equal contribution</p>
                        </div>
                        <div class="column has-text-centered">
                            <!-- ArXiv link -->
                            <span class="link-block">
                                <a target="_blank" href="https://www.omitted.link/" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon"><i class="fas fa-file"></i></span>
                                    <span>ArXiv</span>
                                </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                                <a target="_blank" href="https://www.omitted.link/" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon"><i class="fab fa-github"></i></span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser and Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <div style="text-align: center;">
                <img src="static/images/problem.png" alt="Description of the image" style="width: 70%; height: auto;">
            </div>
            <br>
            <br>
            <!-- Abstract -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            We introduce <b>L</b>atent <b>A</b>ction <b>P</b>retraining for general <b>A</b>ction models (LAPA), the
                            first unsupervised method for pretraining Vision-Language-Action (VLA) models 
                            without ground-truth robot action labels. Existing Vision-Language-Action
                            models require action labels typically collected by human teleoperators during
                            pretraining, which significantly limits possible data sources and scale. In this
                            work, we propose a method to learn from internet-scale videos that do not have
                            robot action labels. We first train an action quantization model leveraging VQ-
                            VAE-based objective to learn discrete latent actions between image frames, then
                            pretrain a latent VLA model to predict these latent actions from observations and
                            task descriptions, and finally finetune the VLA on small-scale robot manipulation
                            data to map from latent to robot actions. Experimental results demonstrate that
                            our method significantly outperforms existing techniques that train robot manipulation 
                            policies from large-scale videos. Furthermore, it outperforms the state-of-
                            the-art VLA model trained with robotic action labels on real-world manipulation
                            tasks that require language conditioning, generalization to unseen objects, and
                            semantic generalization to unseen instructions. Training only on human manipulation 
                            videos also shows positive transfer, opening up the potential for leveraging
                            web-scale data for robotics foundation model. 
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Abstract -->
            <h2>Overview of LAPA</h2>
            <div style="text-align: center;">
                <img src="static/images/method.png" alt="Description of the image" style="width: 100%; height: auto;">
            </div>
        </div>
    </section>

    <!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>
    <!-- /BibTeX -->

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
                            made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- /Footer -->

</body>
</html>
